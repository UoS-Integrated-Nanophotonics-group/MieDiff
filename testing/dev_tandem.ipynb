{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pymiediff as pmd\n",
    "import torch\n",
    "import numpy as np\n",
    "import h5py\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_wavelength = 380  # nm\n",
    "ending_wavelength = 750  # nm\n",
    "\n",
    "N_pt_test = 250\n",
    "\n",
    "wl = torch.linspace(\n",
    "    starting_wavelength,\n",
    "    ending_wavelength,\n",
    "    N_pt_test,\n",
    "    dtype=torch.double,\n",
    "    requires_grad=False,\n",
    ")\n",
    "\n",
    "k0 = 2 * torch.pi / wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define range of starting parameter combinations\n",
    "r_c_min, r_c_max = 10.0, 20.0\n",
    "r_s_min, r_s_max = 45.0, 55.0\n",
    "n_c_min, n_c_max = 2.0 + 0.1j, 2.0 + 0.1j\n",
    "n_s_min, n_s_max = 5.0 + 0.2j, 5.0 + 0.2j\n",
    "\n",
    "# define number of starting parameter combinations\n",
    "NumComb = 1000\n",
    "\n",
    "r_c, r_s, n_c, n_s = pmd.seedComb(\n",
    "    r_c_min,\n",
    "    r_c_max,\n",
    "    r_s_min,\n",
    "    r_s_max,\n",
    "    n_c_min,\n",
    "    n_c_max,\n",
    "    n_s_min,\n",
    "    n_s_max,\n",
    "    NumComb=NumComb,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_section = np.zeros((NumComb, N_pt_test), dtype=np.float32)\n",
    "\n",
    "# # Compute cross-section iteratively\n",
    "# for i in range(NumComb):\n",
    "#     cross_section[i] = pmd.farfield.cross_sections(\n",
    "#         k0=k0,\n",
    "#         r_c=r_c[i],\n",
    "#         eps_c=n_c[i]**2,\n",
    "#         r_s=r_s[i],\n",
    "#         eps_s=n_s[i]**2,\n",
    "#         eps_env=1,\n",
    "#     )['q_sca']\n",
    "#     if i % 50 == 0:\n",
    "#         print(f\"{i}/{NumComb}\")\n",
    "\n",
    "# # Save to HDF5 file\n",
    "# h5_path = \"dataset.h5\"\n",
    "# with h5py.File(h5_path, \"w\") as f:\n",
    "#     f.create_dataset(\"r_c\", data=r_c)\n",
    "#     f.create_dataset(\"r_s\", data=r_s)\n",
    "#     f.create_dataset(\"n_c\", data=n_c)\n",
    "#     f.create_dataset(\"n_s\", data=n_s)\n",
    "#     f.create_dataset(\"cross_section\", data=cross_section)\n",
    "\n",
    "# print(f\"Dataset saved to {h5_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TorchStandardScaler:\n",
    "    def __init__(self):\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    def fit(self, data):\n",
    "        data = torch.tensor(data, dtype=torch.double)\n",
    "        self.mean = data.mean(dim=0)\n",
    "        self.std = data.std(dim=0)\n",
    "        self.std[self.std == 0] = 1.0  # Avoid division by zero\n",
    "\n",
    "    def transform(self, data):\n",
    "        data = torch.tensor(data, dtype=torch.double)\n",
    "        data = data.clone().detach().requires_grad_(True) #torch.tensor(data, dtype=torch.double) #\n",
    "        return (data - self.mean) / self.std\n",
    "\n",
    "    def inverse_transform(self, data):\n",
    "        data = torch.tensor(data, dtype=torch.double)\n",
    "        data = data.clone().detach().requires_grad_(True) #torch.tensor(data, dtype=torch.double)\n",
    "        return data * self.std + self.mean\n",
    "\n",
    "class CoreShellDataset(Dataset):\n",
    "    def __init__(self, h5_file, fit_scalers=True, x_scaler=None, y_scaler=None):\n",
    "        super().__init__()\n",
    "        self.h5_file = h5_file\n",
    "\n",
    "        # Open the file to get dataset sizes (but don't keep it open)\n",
    "        with h5py.File(h5_file, \"r\") as f:\n",
    "            self.length = len(f[\"r_c\"])\n",
    "            if fit_scalers:\n",
    "                # Load all data to fit scalers\n",
    "                x_data = np.stack([f[\"r_c\"][:], f[\"r_s\"][:], f[\"n_c\"][:].real, f[\"n_c\"][:].imag, f[\"n_s\"][:].real, f[\"n_s\"][:].imag], axis=1)\n",
    "                y_data = f[\"cross_section\"][:]\n",
    "\n",
    "                self.x_scaler = TorchStandardScaler()\n",
    "                self.y_scaler = TorchStandardScaler()\n",
    "\n",
    "                self.x_scaler.fit(torch.tensor(x_data, dtype=torch.double))\n",
    "                self.y_scaler.fit(torch.tensor(y_data, dtype=torch.double))\n",
    "            else:\n",
    "                self.x_scaler = x_scaler\n",
    "                self.y_scaler = y_scaler\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.h5_file, \"r\") as f:\n",
    "            r_c = f[\"r_c\"][idx]\n",
    "            r_s = f[\"r_s\"][idx]\n",
    "            n_c_re = f[\"n_c\"][idx].real\n",
    "            n_c_im = f[\"n_c\"][idx].imag\n",
    "            n_s_re = f[\"n_s\"][idx].real\n",
    "            n_s_im = f[\"n_s\"][idx].imag\n",
    "            cross_section = f[\"cross_section\"][idx]\n",
    "\n",
    "        x = np.array([r_c, r_s, n_c_re, n_c_im, n_s_re, n_s_im]).reshape(1, -1)\n",
    "        y = np.array(cross_section).reshape(1, -1)\n",
    "\n",
    "        # Scale data using PyTorch scaler\n",
    "        x = self.x_scaler.transform(x).flatten()\n",
    "        y = self.y_scaler.transform(cross_section.reshape(1, -1)).flatten()\n",
    "\n",
    "        return x, y\n",
    "\n",
    "# Usage example\n",
    "h5_path = \"dataset.h5\"\n",
    "dataset = CoreShellDataset(h5_path)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = dataset.x_scaler\n",
    "y_scaler = dataset.y_scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using the scalers on new data\n",
    "new_x = np.array([[0.5, 0.7, 1.5, 2.0, 1.5, 2.0]])\n",
    "scaled_x = x_scaler.transform(new_x)\n",
    "original_x = x_scaler.inverse_transform(scaled_x)\n",
    "\n",
    "new_y = np.random.rand(1, 250)  # Example y data with size 20\n",
    "scaled_y = y_scaler.transform(new_y)\n",
    "original_y = y_scaler.inverse_transform(scaled_y)\n",
    "\n",
    "print(\"Scaled x:\", scaled_x)\n",
    "print(\"Original x:\", original_x)\n",
    "print(\"Scaled y:\", scaled_y)\n",
    "print(\"Original y:\", original_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=250, output_dim=6, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.rand((32, 4))\n",
    "test_shape = test.shape\n",
    "print(test_shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def vector_batch(x_pred):\n",
    "\n",
    "\n",
    "    batch_shape = x_pred.shape\n",
    "    # [r_c, r_s, n_c_re, n_c_im, n_s_re, n_s_im]\n",
    "    y_pred_temp = []\n",
    "\n",
    "    for i in range(batch_shape[0]):\n",
    "        x_batch = x_pred[i, :]\n",
    "        r_c_i, r_s_i, n_c_re_i, n_c_im_i, n_s_re_i, n_s_im_i = x_batch[0], x_batch[1], x_batch[2], x_batch[3], x_batch[4], x_batch[5]\n",
    "\n",
    "        #print(\"x\", x_batch)\n",
    "\n",
    "        y_batch = pmd.farfield.cross_sections(\n",
    "            k0=k0,\n",
    "            r_c=r_c_i,\n",
    "            eps_c=(n_c_re_i+1j*n_c_im_i) ** 2,\n",
    "            r_s=r_s_i,\n",
    "            eps_s=(n_s_re_i+1j*n_s_im_i) ** 2,\n",
    "            eps_env=1,\n",
    "        )[\"q_sca\"]\n",
    "\n",
    "\n",
    "\n",
    "        # y_batch = y_batch.to(dtype=torch.double)  # Set dtype\n",
    "        # y_batch[torch.isnan(y_batch)] = 0  # Replace NaNs with zero\n",
    "\n",
    "        #print(\"y\", y_batch[:10])\n",
    "        y_pred_temp.append(y_batch)\n",
    "\n",
    "    # print(y_pred_temp)\n",
    "\n",
    "    y_pred = torch.stack(y_pred_temp)\n",
    "\n",
    "\n",
    "\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def train_model(model, dataloader, num_epochs=20, lr=1e-5, device=\"cpu\"):#\"cuda\" if torch.cuda.is_available() else \n",
    "    model.to(device)\n",
    "    model.double()\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        for x_batch, y_batch in dataloader:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Inverse model\n",
    "            x_pred = model(y_batch)\n",
    "            #print(x_pred.shape)\n",
    "            # Forward model\n",
    "\n",
    "            x_pred_scaled = x_scaler.inverse_transform(x_pred)\n",
    "\n",
    "            y_pred = vector_batch(x_pred_scaled)#\n",
    "\n",
    "            y_pred_scaled = y_scaler.transform(y_pred)\n",
    "            # print(\"y test\", y_batch[0, :10])\n",
    "            # print(\"vVv Inverse network vVv\")\n",
    "            # print(\"x pred\", x_pred_scaled[0, :])\n",
    "            # print(\"vVv Forward Mie solver vVv\")\n",
    "            # print(\"y pred\", y_pred[0, :10])\n",
    "\n",
    "            loss = criterion(y_pred_scaled, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss}\")\n",
    "\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "h5_path = \"dataset.h5\"\n",
    "\n",
    "train_dataset = CoreShellDataset(h5_path)\n",
    "# scaler = train_dataset.scaler  # Save scaler for later use\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = MLP()\n",
    "train_model(mlp_model, dataloader, num_epochs=50, lr=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a sample from the dataset\n",
    "x_sample, y_sample = train_dataset[0]  # Example input\n",
    "x_sample = x_sample.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Run inference\n",
    "mlp_model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = mlp_model(x_sample)\n",
    "\n",
    "# Convert prediction back to original scale\n",
    "y_pred_original = scaler.inverse_transform(y_pred.numpy())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
