{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import spherical_jn, spherical_yn\n",
    "\n",
    "from scipy import special\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the torch functions for the spherical bessel functions both forward and backward (its gradient), as the equations for the scattering coeffents invole the derivative of the spherical bessel functions we also need the second deriviative of them. These can be found starting from the reccurce relations,\n",
    "\n",
    "$$\n",
    "f_n^\\prime(z) = f_{n-1}(z) - \\frac{n+1}{z}f_n(z),\n",
    "$$\n",
    "and,\n",
    "$$\n",
    "f_n^\\prime(z) = - f_{n+1}(z) + \\frac{n}{z}f_n(z),\n",
    "$$\n",
    "\n",
    "where $f_n(z)$ is any spherical bessel function. First for later use substitie $n = n+1$ into the first equation to get,\n",
    "\n",
    "$$\n",
    "f_{n+1}^\\prime(z) = f_{n}(z) - \\frac{n+2}{z}f_{n+1}(z)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Taking the derivative of the secound equation,\n",
    "\n",
    "$$\n",
    "\\frac{d^2}{dz^2}f_n(z) = - \\frac{d}{dz}f_{n+1}(z) + n\\frac{d}{dz} \\left( \\frac{f_n(z)}{z} \\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_n^{\\prime\\prime}(z) = - f_{n+1}^\\prime(z) + n \\left( \\frac{f_n^\\prime(z)z-f_n(z)}{z^2} \\right).\n",
    "$$\n",
    "Reagrange this to \n",
    "\n",
    "$$\n",
    "z^2f_n^{\\prime\\prime}(z) = -z^2f_{n+1}^\\prime(z) + nzf_{n}^\\prime - nf_n(z),\n",
    "$$\n",
    "and then substituie the modified first equation and the second equation to get,\n",
    "$$\n",
    "z^2f_n^{\\prime\\prime}(z) = -z^2\\left(f_{n}(z) - \\frac{n+2}{z}f_{n+1}(z)\\right) + nz \\left( - f_{n+1}(z) + \\frac{n}{z}f_n(z) \\right) - nf_n(z).\n",
    "$$\n",
    "Reagrangr this to get the equation for $f_n^{\\prime\\prime}(z)$,\n",
    "\n",
    "$$\n",
    "z^2f_n^{\\prime\\prime}(z) = f_n(z) \\left ( -z^2 + n^2 - n\\right) + f_{n+1}(z) \\left ( z(n+2) -nz \\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_n^{\\prime\\prime}(z) = \\frac{1}{z^2} \\left [ (n^2 - n - z^2)f_n(z) + 2z f_{n+1}(z)     \\right ].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_jn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_jn(n, input.numpy()))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy(spherical_jn(ctx.n, grad_output, derivative = True))\n",
    "\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "\n",
    "\n",
    "class torch_jn_der(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_jn(n, input.numpy(), derivative = True))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n.detach().numpy()**2 - ctx.n.detach().numpy() - grad_output**2)*spherical_jn(ctx.n.detach().numpy(), grad_output) + 2*grad_output*spherical_jn(ctx.n.detach().numpy() + 1, grad_output)) )\n",
    "\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "\n",
    "\n",
    "class torch_yn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_yn(n, input.numpy()))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy(spherical_yn(ctx.n, grad_output, derivative = True))\n",
    "\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "\n",
    "\n",
    "class torch_yn_der(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_yn(n, input.numpy(), derivative = True))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n.detach().numpy()**2 - ctx.n.detach().numpy() - grad_output**2)*spherical_yn(ctx.n.detach().numpy(), grad_output) + 2*grad_output*spherical_yn(ctx.n.detach().numpy() + 1, grad_output)) )\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions we define the Riccati-Bessel Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sph_jn = torch_jn.apply\n",
    "sph_yn = torch_yn.apply\n",
    "\n",
    "sph_jn_der = torch_jn_der.apply\n",
    "sph_yn_der = torch_yn_der.apply\n",
    "\n",
    "def sph_h1n(z, n):\n",
    "    return sph_jn(z, n) + 1j*sph_yn(z, n)\n",
    "\n",
    "def sph_h1n_der(z, n):\n",
    "    return sph_jn_der(z, n) + 1j*sph_yn_der(z, n)\n",
    "\n",
    "def psi(z, n):\n",
    "    return z*sph_jn(z,n)\n",
    "\n",
    "def chi(z, n):\n",
    "    return -z*sph_yn(z, n)\n",
    "\n",
    "def xi(z, n):\n",
    "    return z*sph_h1n(z, n)\n",
    "\n",
    "def psi_der(z, n):\n",
    "    return sph_jn(z,n) + z*sph_jn_der(z,n)\n",
    "\n",
    "def chi_der(z, n):\n",
    "    return -sph_yn(z,n) - z*sph_yn_der(z,n)\n",
    "\n",
    "def xi_der(z, n):\n",
    "    return sph_h1n(z,n) + z*sph_h1n_der(z,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def An(x, n, m1, m2):\n",
    "    return (m2*psi(m2*x, n)*psi_der(m1*x, n) - m1*psi_der(m2*x, n)*psi(m1*x, n))/(m2*chi(m2*x, n)*psi_der(m1*x, n) - m1*chi_der(m2*x, n)*psi(m1*x, n))\n",
    "\n",
    "def Bn(x, n, m1, m2):\n",
    "    return (m2*psi(m1*x, n)*psi_der(m2*x, n) - m1*psi(m2*x, n)*psi_der(m1*x, n))/(m2*chi_der(m2*x, n)*psi(m1*x, n) - m1*psi_der(m1*x, n)*chi(m2*x, n))\n",
    "\n",
    "def an(x, y, n, m1, m2):\n",
    "    return (psi(y, n)*(psi_der(m2*y, n) - An(x, n, m1, m2)*chi_der(m2*y, n)) - m2*psi_der(y, n)*(psi(m2*y, n) - An(x, n, m1, m2)*chi(m2*y, n)))/(xi(y, n)*(psi_der(m2*y, n) - An(x, n, m1, m2)*chi_der(m2*y, n)) - m2*xi_der(y, n)*(psi(m2*y, n) - An(x, n, m1, m2)*chi(m2*y, n)))\n",
    "\n",
    "def bn(x, y, n, m1, m2):\n",
    "    return (m2*psi(y, n)*(psi_der(m2*y, n) - Bn(x, n, m1, m2)*chi_der(m2*y, n)) - psi_der(y, n)*(psi(m2*y, n) - Bn(x, n, m1, m2)*chi(m2*y, n)))/(m2*xi(y, n)*(psi_der(m2*y, n) - Bn(x, n, m1, m2)*chi_der(m2*y, n)) - xi_der(y, n)*(psi(m2*y, n) - Bn(x, n, m1, m2)*chi(m2*y, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wlRes = 10\n",
    "\n",
    "wl = np.linspace(500,550, wlRes)#500.0  # wavelength in nm\n",
    "\n",
    "wl = np.reshape(wl,newshape=(wlRes,1))\n",
    "#wl = 500\n",
    "#print(wl)\n",
    "\n",
    "r_core = 80.0\n",
    "r_shell = r_core + 100.0\n",
    "\n",
    "n_env = 1\n",
    "n_core = 4\n",
    "n_shell = 0.1   + .7j\n",
    "\n",
    "mu_env = 1\n",
    "mu_core = 1\n",
    "mu_shell = 1\n",
    "\n",
    "dtype = torch.complex64\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "n_max = 5\n",
    "k = 2 * np.pi / (wl / n_env)\n",
    "\n",
    "#print(k.shape)\n",
    "\n",
    "m1 = n_core / n_env\n",
    "m2 = n_shell / n_env\n",
    "x = k * r_core\n",
    "y = k * r_shell\n",
    "\n",
    "\n",
    "#x = np.repeat(x, n_max, axis = 1 )\n",
    "#y = np.repeat(y, n_max, axis = 1 )\n",
    "\n",
    "#print(x.shape)\n",
    "\n",
    "#x = ka where a is the core raduis\n",
    "#y = kb where b is the raduis of the shell\n",
    "\n",
    "#x = torch.linspace(1, 2, 3, device=device, dtype=dtype, requires_grad=True)\n",
    "#y = torch.linspace(1, 2, 3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "x = torch.tensor(x, requires_grad=True)\n",
    "y = torch.tensor(y, requires_grad=True)\n",
    "\n",
    "m1 = torch.tensor(m1, requires_grad=True, dtype=dtype)\n",
    "m2 = torch.tensor(m2, requires_grad=True, dtype=dtype)\n",
    "\n",
    "\n",
    "#m1 = torch.tensor(3.0, requires_grad=True)\n",
    "#m2 = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "#m1 = torch.full(x.shape, 3.0, device=device, dtype=dtype, requires_grad=True)\n",
    "#m2 = torch.full(x.shape, 2.0, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "n1 = torch.tensor([1,2,3,4,5])#np.arange(1, n_max+1)\n",
    "\n",
    "a1= an(x, y, n1, m1, m2)  #Testing with an to begin with\n",
    "b1= bn(x, y, n1, m1, m2)\n",
    "\n",
    "#print(a1[0], a[0])\n",
    "#print(a1.shape, b1.shape)\n",
    "#print(x, x.shape)\n",
    "prefactor1 = 2 / (k**2 * r_shell**2)\n",
    "\n",
    "#print(\"here\",prefactor1.shape)\n",
    "\n",
    "\n",
    "#qext1 = torch.tensor(prefactor1) * torch.sum((2 * n1 + 1) * (a1.real + b1.real),dim=1)\n",
    "#qsca1 = torch.tensor(prefactor1) * torch.sum((2 * n1 + 1) * (a1.real**2 + a1.imag**2 + b1.real**2 + b1.imag**2),dim=1)\n",
    "#qabs1 = qext1 - qsca1\n",
    "\n",
    "\n",
    "#print(torch.reshape(torch.sum((2 * n1 + 1) * (a1.real + b1.real),dim=1), (10,1) ).shape)\n",
    "\n",
    "qext1 = torch.mul( torch.reshape(torch.sum((2 * n1 + 1) * (a1.real + b1.real),dim=1), (wlRes,1) ), torch.tensor(prefactor1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "qsca1 = torch.tensor(prefactor1) * torch.reshape(torch.sum((2 * n1 + 1) * (a1.real**2 + a1.imag**2 + b1.real**2 + b1.imag**2),dim=1), (wlRes,1) )\n",
    "qabs1 = qext1 - qsca1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Mismatch in shape: grad_output[0] has a shape of torch.Size([10, 1]) and output[0] has a shape of torch.Size([10, 5]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#Is this the correct way to find the gradient of an wrt x, y, m1 and m2? This works with no error.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#Reading these:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# you must call backward only once!\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[43ma1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# calling backward again will sum the gradients again, so it will change (falsify) previously calculted gradients\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# output.backward(y, retain_graph = True, create_graph = True)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# output.backward(m1, retain_graph = True, create_graph = True)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# output.backward(m2, retain_graph = True, create_graph = True)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mddX\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39mgrad)\n",
      "File \u001b[0;32m~/anaconda3/envs/newnewcomer/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/newnewcomer/lib/python3.12/site-packages/torch/autograd/__init__.py:260\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    251\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    252\u001b[0m     (inputs,)\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m    257\u001b[0m )\n\u001b[1;32m    259\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[0;32m--> 260\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[0;32m~/anaconda3/envs/newnewcomer/lib/python3.12/site-packages/torch/autograd/__init__.py:104\u001b[0m, in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     87\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf `is_grads_batched=True`, we interpret the first \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     88\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension of each grad_output as the batch dimension. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatched, consider using vmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m         )\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    105\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in shape: grad_output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grads\u001b[38;5;241m.\u001b[39mindex(grad))\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(grad_shape)\n\u001b[1;32m    109\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and output[\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    110\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(outputs\u001b[38;5;241m.\u001b[39mindex(out))\n\u001b[1;32m    111\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m] has a shape of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(out_shape)\n\u001b[1;32m    113\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         )\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex \u001b[38;5;241m!=\u001b[39m grad\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_complex:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFor complex Tensors, both grad_output and output\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are required to have the same dtype.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Mismatch in shape: grad_output[0] has a shape of torch.Size([10, 1]) and output[0] has a shape of torch.Size([10, 5])."
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    #Is this the correct way to find the gradient of an wrt x, y, m1 and m2? This works with no error.\n",
    "    #Reading these:\n",
    "    # https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/7\n",
    "    # https://discuss.pytorch.org/t/second-order-derivatives-of-loss-function/71797/3\n",
    "    #It seems like this may be wrong\n",
    "\n",
    "    # you must call backward only once!\n",
    "    a1.backward([x, y, m1, m2], retain_graph = True, create_graph = True)\n",
    "\n",
    "    # calling backward again will sum the gradients again, so it will change (falsify) previously calculted gradients\n",
    "    # output.backward(y, retain_graph = True, create_graph = True)\n",
    "    # output.backward(m1, retain_graph = True, create_graph = True)\n",
    "    # output.backward(m2, retain_graph = True, create_graph = True)\n",
    "\n",
    "    print(\"ddX\", x.grad)\n",
    "    print(\"ddY\", y.grad)\n",
    "    print(\"ddm1\", m1.grad)\n",
    "    print(\"ddm2\", m2.grad)\n",
    "\n",
    "\n",
    "    # if you want to use the autograd interface, you need to tell him explicitly the shape and dtype of the output\n",
    "    # (see https://discuss.pytorch.org/t/what-is-the-difference-between-autograd-backward-and-autograd-grad/74663/3)\n",
    "    #x_grad = torch.autograd.grad(output, x, create_graph=True, grad_outputs=torch.ones(3, dtype=dtype))\n",
    "    #print(\"ddX via autograd:\", x_grad)\n",
    "\n",
    "    # then, if you created a graph, you can autograd this again (it should also work with the \"backward\" API...)\n",
    "    # I'm not entirely sure about this syntax though, but anyways we need to verify everything with numerical differentiation.\n",
    "    #x_grad_grad = torch.autograd.grad(x_grad, x, grad_outputs=torch.ones(3, dtype=dtype))\n",
    "    #print(\"d2 dX2:\", x_grad_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newnewcomer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
