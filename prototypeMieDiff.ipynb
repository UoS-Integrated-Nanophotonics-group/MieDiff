{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import spherical_jn, spherical_yn\n",
    "\n",
    "from scipy import special\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the torch functions for the spherical bessel functions both forward and backward (its gradient), as the equations for the scattering coeffents invole the derivative of the spherical bessel functions we also need the second deriviative of them. These can be found starting from the reccurce relations,\n",
    "\n",
    "$$\n",
    "f_n^\\prime(z) = f_{n-1}(z) - \\frac{n+1}{z}f_n(z),\n",
    "$$\n",
    "and,\n",
    "$$\n",
    "f_n^\\prime(z) = - f_{n+1}(z) + \\frac{n}{z}f_n(z),\n",
    "$$\n",
    "\n",
    "where $f_n(z)$ is any spherical bessel function. First for later use substitie $n = n+1$ into the first equation to get,\n",
    "\n",
    "$$\n",
    "f_{n+1}^\\prime(z) = f_{n}(z) - \\frac{n+2}{z}f_{n+1}(z)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Taking the derivative of the secound equation,\n",
    "\n",
    "$$\n",
    "\\frac{d^2}{dz^2}f_n(z) = - \\frac{d}{dz}f_{n+1}(z) + n\\frac{d}{dz} \\left( \\frac{f_n(z)}{z} \\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_n^{\\prime\\prime}(z) = - f_{n+1}^\\prime(z) + n \\left( \\frac{f_n^\\prime(z)z-f_n(z)}{z^2} \\right).\n",
    "$$\n",
    "Reagrange this to \n",
    "\n",
    "$$\n",
    "z^2f_n^{\\prime\\prime}(z) = -z^2f_{n+1}^\\prime(z) + nzf_{n}^\\prime - nf_n(z),\n",
    "$$\n",
    "and then substituie the modified first equation and the second equation to get,\n",
    "$$\n",
    "z^2f_n^{\\prime\\prime}(z) = -z^2\\left(f_{n}(z) - \\frac{n+2}{z}f_{n+1}(z)\\right) + nz \\left( - f_{n+1}(z) + \\frac{n}{z}f_n(z) \\right) - nf_n(z).\n",
    "$$\n",
    "Reagrangr this to get the equation for $f_n^{\\prime\\prime}(z)$,\n",
    "\n",
    "$$\n",
    "z^2f_n^{\\prime\\prime}(z) = f_n(z) \\left ( -z^2 + n^2 - n\\right) + f_{n+1}(z) \\left ( z(n+2) -nz \\right),\n",
    "$$\n",
    "\n",
    "$$\n",
    "f_n^{\\prime\\prime}(z) = \\frac{1}{z^2} \\left [ (n^2 - n - z^2)f_n(z) + 2z f_{n+1}(z)     \\right ].\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_jn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_jn(n, input.numpy()))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy(spherical_jn(ctx.n, grad_output, derivative = True))\n",
    "\n",
    "        return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "    \n",
    "\n",
    "class torch_jn_der(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_jn(n, input.numpy(), derivative = True))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n**2 - ctx.n - grad_output**2)*spherical_jn(ctx.n, grad_output) + 2*grad_output*spherical_jn(ctx.n + 1, grad_output)) )\n",
    "\n",
    "        return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "\n",
    "\n",
    "class torch_yn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_yn(n, input.numpy()))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy(spherical_yn(ctx.n, grad_output, derivative = True))\n",
    "\n",
    "        return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "    \n",
    "\n",
    "class torch_yn_der(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_yn(n, input.numpy(), derivative = True))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n**2 - ctx.n - grad_output**2)*spherical_yn(ctx.n, grad_output) + 2*grad_output*spherical_yn(ctx.n + 1, grad_output)) )\n",
    "\n",
    "        return torch.as_tensor(grad_input, dtype=torch.complex64), None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these functions we define the Riccati-Bessel Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sph_jn = torch_jn.apply\n",
    "sph_yn = torch_yn.apply\n",
    "\n",
    "sph_jn_der = torch_jn_der.apply\n",
    "sph_yn_der = torch_yn_der.apply\n",
    "\n",
    "def sph_h1n(z, n):\n",
    "    return sph_jn(z, n) + 1j*sph_yn(z, n)\n",
    "\n",
    "def sph_h1n_der(z, n):\n",
    "    return sph_jn_der(z, n) + 1j*sph_yn_der(z, n)\n",
    "\n",
    "def psi(z, n):\n",
    "    return z*sph_jn(z,n)\n",
    "\n",
    "def chi(z, n):\n",
    "    return -z*sph_yn(z, n)\n",
    "\n",
    "def xi(z, n):\n",
    "    return z*sph_h1n(z, n)\n",
    "\n",
    "def psi_der(z, n):\n",
    "    return sph_jn(z,n) + z*sph_jn_der(z,n)\n",
    "\n",
    "def chi_der(z, n):\n",
    "    return -sph_yn(z,n) - z*sph_yn_der(z,n)\n",
    "\n",
    "def xi_der(z, n):\n",
    "    return sph_h1n(z,n) + z*sph_h1n_der(z,n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from these we define the scattering coeffients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def An(x, n, m1, m2):\n",
    "    return (m2*psi(m2*x, n)*psi_der(m1*x, n) - m1*psi_der(m2*x, n)*psi(m1*x, n))/(m2*chi(m2*x, n)*psi_der(m1*x, n) - m1*chi_der(m2*x, n)*psi(m1*x, n))\n",
    "\n",
    "def Bn(x, n, m1, m2):\n",
    "    return (m2*psi(m1*x, n)*psi_der(m2*x, n) - m1*psi(m2*x, n)*psi_der(m1*x, n))/(m2*chi_der(m2*x, n)*psi(m1*x, n) - m1*psi_der(m1*x, n)*chi(m2*x, n))\n",
    "\n",
    "def an(x, y, n, m1, m2):\n",
    "    return (psi(y, n)*(psi_der(m2*y, n) - An(x, n, m1, m2)*chi_der(m2*y, n)) - m2*psi_der(y, n)*(psi(m2*y, n) - An(x, n, m1, m2)*chi(m2*y, n)))/(xi(y, n)*(psi_der(m2*y, n) - An(x, n, m1, m2)*chi_der(m2*y, n)) - m2*xi_der(y, n)*(psi(m2*y, n) - An(x, n, m1, m2)*chi(m2*y, n)))\n",
    "\n",
    "def bn(x, y, n, m1, m2):\n",
    "    return (m2*psi(y, n)*(psi_der(m2*y, n) - Bn(x, n, m1, m2)*chi_der(m2*y, n)) - psi_der(y, n)*(psi(m2*y, n) - Bn(x, n, m1, m2)*chi(m2*y, n)))/(m2*xi(y, n)*(psi_der(m2*y, n) - Bn(x, n, m1, m2)*chi_der(m2*y, n)) - xi_der(y, n)*(psi(m2*y, n) - Bn(x, n, m1, m2)*chi(m2*y, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.complex64\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#x = ka where a is the core raduis\n",
    "#y = kb where b is the raduis of the shell\n",
    "\n",
    "x = torch.linspace(1, 2, 3, device=device, dtype=dtype, requires_grad=True)\n",
    "y = torch.linspace(1, 2, 3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "#m1 = torch.tensor(3.0, requires_grad=True)\n",
    "#m2 = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "m1 = torch.full(x.shape, 3.0, device=device, dtype=dtype, requires_grad=True)\n",
    "m2 = torch.full(x.shape, 2.0, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "n = 3\n",
    "\n",
    "output = an(x, y, n, m1, m2)  #Testing with an to begin with\n",
    "\n",
    "print(output, output.shape)\n",
    "print(x, x.shape)\n",
    "if True:\n",
    "    #Is this the correct way to find the gradient of an wrt x, y, m1 and m2? This works with no error.\n",
    "    #Reading these:\n",
    "    # https://discuss.pytorch.org/t/how-to-calculate-2nd-derivative-of-a-likelihood-function/15085/7\n",
    "    # https://discuss.pytorch.org/t/second-order-derivatives-of-loss-function/71797/3\n",
    "    #It seems like this may be wrong\n",
    "    \n",
    "    # you must call backward only once!\n",
    "    output.backward([x, y, m1, m2], retain_graph = True, create_graph = True)\n",
    "    \n",
    "    # calling backward again will sum the gradients again, so it will change (falsify) previously calculted gradients\n",
    "    # output.backward(y, retain_graph = True, create_graph = True)\n",
    "    # output.backward(m1, retain_graph = True, create_graph = True)\n",
    "    # output.backward(m2, retain_graph = True, create_graph = True)\n",
    "\n",
    "    print(\"ddX\", x.grad)\n",
    "    print(\"ddY\", y.grad)\n",
    "    print(\"ddm1\", m1.grad)\n",
    "    print(\"ddm2\", m2.grad)\n",
    "    \n",
    "    \n",
    "    # if you want to use the autograd interface, you need to tell him explicitly the shape and dtype of the output\n",
    "    # (see https://discuss.pytorch.org/t/what-is-the-difference-between-autograd-backward-and-autograd-grad/74663/3)\n",
    "    x_grad = torch.autograd.grad(output, x, create_graph=True, grad_outputs=torch.ones(3, dtype=dtype))\n",
    "    print(\"ddX via autograd:\", x_grad)\n",
    "    \n",
    "    # then, if you created a graph, you can autograd this again (it should also work with the \"backward\" API...)\n",
    "    # I'm not entirely sure about this syntax though, but anyways we need to verify everything with numerical differentiation.\n",
    "    x_grad_grad = torch.autograd.grad(x_grad, x, grad_outputs=torch.ones(3, dtype=dtype))\n",
    "    print(\"d2 dX2:\", x_grad_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newnewcomer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
