{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import spherical_jn, spherical_yn\n",
    "\n",
    "from scipy import special\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Function\n",
    "from torch.nn.modules.module import Module\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class torch_jn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_jn(n, input.numpy()))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy(spherical_jn(ctx.n, grad_output, derivative = True))\n",
    "\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "\n",
    "\n",
    "class torch_jn_der(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_jn(n, input.numpy(), derivative = True))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n.detach().numpy()**2 - ctx.n.detach().numpy() - grad_output**2)*spherical_jn(ctx.n.detach().numpy(), grad_output) + 2*grad_output*spherical_jn(ctx.n.detach().numpy() + 1, grad_output)) )\n",
    "\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "\n",
    "\n",
    "class torch_yn(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_yn(n, input.numpy()))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy(spherical_yn(ctx.n, grad_output, derivative = True))\n",
    "\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None\n",
    "\n",
    "\n",
    "class torch_yn_der(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, n):\n",
    "        input = input.detach()\n",
    "        result = torch.from_numpy(spherical_yn(n, input.numpy(), derivative = True))\n",
    "        ctx.save_for_backward(result)\n",
    "        ctx.n = n #n is not learnable so can just save in ctx\n",
    "\n",
    "        return result\n",
    "        #return torch.as_tensor(result, dtype=torch.complex64)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_output = grad_output.detach()\n",
    "        input = ctx.saved_tensors\n",
    "        grad_output = grad_output.numpy()\n",
    "        grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n.detach().numpy()**2 - ctx.n.detach().numpy() - grad_output**2)*spherical_yn(ctx.n.detach().numpy(), grad_output) + 2*grad_output*spherical_yn(ctx.n.detach().numpy() + 1, grad_output)) )\n",
    "        return grad_input, None\n",
    "        #return torch.as_tensor(grad_input, dtype=torch.complex64), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.00498751e+02 -2.54950111e+01 -1.15999172e+01]\n",
      " [-3.00501248e+03 -3.77524834e+02 -1.12814717e+02]\n",
      " [-1.50150125e+05 -9.41262583e+03 -1.86864537e+03]\n",
      " [-1.05075038e+07 -3.29064379e+05 -4.34889106e+04]\n",
      " [-9.45525188e+08 -1.47984844e+07 -1.30279867e+06]]\n",
      "[[-1.00498751e+02 -2.54950111e+01 -1.15999172e+01]\n",
      " [-3.00501248e+03 -3.77524834e+02 -1.12814717e+02]\n",
      " [-1.50150125e+05 -9.41262583e+03 -1.86864537e+03]\n",
      " [-1.05075038e+07 -3.29064379e+05 -4.34889106e+04]\n",
      " [-9.45525188e+08 -1.47984844e+07 -1.30279867e+06]]\n"
     ]
    }
   ],
   "source": [
    "# Testing shapes\n",
    "n = np.array([1,2,3,4,5])\n",
    "n = np.reshape(n,(5,1))\n",
    "\n",
    "z = np.array([0.1,0.2,0.3])\n",
    "print(spherical_yn(n,z))\n",
    "\n",
    "\n",
    "n = np.array([1,2,3,4,5])\n",
    "\n",
    "\n",
    "z = np.array([0.1,0.2,0.3])\n",
    "z = np.reshape(z,(3,1))\n",
    "print(spherical_yn(n,z).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sph_jn = torch_jn.apply\n",
    "sph_yn = torch_yn.apply\n",
    "\n",
    "sph_jn_der = torch_jn_der.apply\n",
    "sph_yn_der = torch_yn_der.apply\n",
    "\n",
    "def sph_h1n(z, n):\n",
    "    return sph_jn(z, n) + 1j*sph_yn(z, n)\n",
    "\n",
    "def sph_h1n_der(z, n):\n",
    "    return sph_jn_der(z, n) + 1j*sph_yn_der(z, n)\n",
    "\n",
    "def psi(z, n):\n",
    "    return z*sph_jn(z,n)\n",
    "\n",
    "def chi(z, n):\n",
    "    return -z*sph_yn(z, n)\n",
    "\n",
    "def xi(z, n):\n",
    "    return z*sph_h1n(z, n)\n",
    "\n",
    "def psi_der(z, n):\n",
    "    return sph_jn(z,n) + z*sph_jn_der(z,n)\n",
    "\n",
    "def chi_der(z, n):\n",
    "    return -sph_yn(z,n) - z*sph_yn_der(z,n)\n",
    "\n",
    "def xi_der(z, n):\n",
    "    return sph_h1n(z,n) + z*sph_h1n_der(z,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def An(k, a, b, n, m1, m2):\n",
    "    return (m2*psi(m2*k*a, n)*psi_der(m1*k*a, n) - m1*psi_der(m2*k*a, n)*psi(m1*k*a, n))/(m2*chi(m2*k*a, n)*psi_der(m1*k*a, n) - m1*chi_der(m2*k*a, n)*psi(m1*k*a, n))\n",
    "\n",
    "def Bn(k, a, n, m1, m2):\n",
    "    return (m2*psi(m1*k*a, n)*psi_der(m2*k*a, n) - m1*psi(m2*k*a, n)*psi_der(m1*k*a, n))/(m2*chi_der(m2*k*a, n)*psi(m1*x, n) - m1*psi_der(m1*k*a, n)*chi(m2*k*a, n))\n",
    "\n",
    "def an(k, a, b, n, m1, m2):\n",
    "    return (psi(k*b, n)*(psi_der(m2*k*b, n) - An(k*a, n, m1, m2)*chi_der(m2*k*b, n)) - m2*psi_der(k*b, n)*(psi(m2*y, n) - An(k*a, n, m1, m2)*chi(m2*k*b, n)))/(xi(y, n)*(psi_der(m2*k*b, n) - An(k*a, n, m1, m2)*chi_der(m2*k*b, n)) - m2*xi_der(k*b, n)*(psi(m2*k*b, n) - An(k*a, n, m1, m2)*chi(m2*k*b, n)))\n",
    "\n",
    "def bn(k, a, b, n, m1, m2):\n",
    "    return (m2*psi(y, n)*(psi_der(m2*k*b, n) - Bn(k*a, n, m1, m2)*chi_der(m2*k*b, n)) - psi_der(k*b, n)*(psi(m2*k*b, n) - Bn(k*a, n, m1, m2)*chi(m2*k*b, n)))/(m2*xi(k*b, n)*(psi_der(m2*k*b, n) - Bn(k*a, n, m1, m2)*chi_der(m2*k*b, n)) - xi_der(k*b, n)*(psi(m2*k*b, n) - Bn(k*a, n, m1, m2)*chi(m2*k*b, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def An(x, n, m1, m2):\n",
    "    return (m2*psi(m2*x, n)*psi_der(m1*x, n) - m1*psi_der(m2*x, n)*psi(m1*x, n))/(m2*chi(m2*x, n)*psi_der(m1*x, n) - m1*chi_der(m2*x, n)*psi(m1*x, n))\n",
    "\n",
    "def Bn(x, n, m1, m2):\n",
    "    return (m2*psi(m1*x, n)*psi_der(m2*x, n) - m1*psi(m2*x, n)*psi_der(m1*x, n))/(m2*chi_der(m2*x, n)*psi(m1*x, n) - m1*psi_der(m1*x, n)*chi(m2*x, n))\n",
    "\n",
    "def an(x, y, n, m1, m2):\n",
    "    return (psi(y, n)*(psi_der(m2*y, n) - An(x, n, m1, m2)*chi_der(m2*y, n)) - m2*psi_der(y, n)*(psi(m2*y, n) - An(x, n, m1, m2)*chi(m2*y, n)))/(xi(y, n)*(psi_der(m2*y, n) - An(x, n, m1, m2)*chi_der(m2*y, n)) - m2*xi_der(y, n)*(psi(m2*y, n) - An(x, n, m1, m2)*chi(m2*y, n)))\n",
    "\n",
    "def bn(x, y, n, m1, m2):\n",
    "    return (m2*psi(y, n)*(psi_der(m2*y, n) - Bn(x, n, m1, m2)*chi_der(m2*y, n)) - psi_der(y, n)*(psi(m2*y, n) - Bn(x, n, m1, m2)*chi(m2*y, n)))/(m2*xi(y, n)*(psi_der(m2*y, n) - Bn(x, n, m1, m2)*chi_der(m2*y, n)) - xi_der(y, n)*(psi(m2*y, n) - Bn(x, n, m1, m2)*chi(m2*y, n)))\n",
    "\n",
    "#def x(k, a):\n",
    "#    return k*a\n",
    "\n",
    "#def y(k, b):\n",
    "#    return k*b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1])\n",
      "here torch.Size([10])\n",
      "tensor([[0.0698+0.j],\n",
      "        [0.0674+0.j],\n",
      "        [0.0650+0.j],\n",
      "        [0.0628+0.j],\n",
      "        [0.0607+0.j],\n",
      "        [0.0586+0.j],\n",
      "        [0.0567+0.j],\n",
      "        [0.0549+0.j],\n",
      "        [0.0531+0.j],\n",
      "        [0.0515+0.j]], dtype=torch.complex128, grad_fn=<MulBackward0>) torch.Size([10, 1])\n",
      "tensor([[0.0611+0.j],\n",
      "        [0.0589+0.j],\n",
      "        [0.0568+0.j],\n",
      "        [0.0549+0.j],\n",
      "        [0.0530+0.j],\n",
      "        [0.0512+0.j],\n",
      "        [0.0495+0.j],\n",
      "        [0.0478+0.j],\n",
      "        [0.0463+0.j],\n",
      "        [0.0448+0.j]], dtype=torch.complex128, grad_fn=<MulBackward0>) torch.Size([10, 1])\n",
      "tensor([[0.0087+0.j],\n",
      "        [0.0085+0.j],\n",
      "        [0.0082+0.j],\n",
      "        [0.0080+0.j],\n",
      "        [0.0077+0.j],\n",
      "        [0.0075+0.j],\n",
      "        [0.0073+0.j],\n",
      "        [0.0070+0.j],\n",
      "        [0.0068+0.j],\n",
      "        [0.0066+0.j]], dtype=torch.complex128, grad_fn=<SubBackward0>) torch.Size([10, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3524219/2315991000.py:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, requires_grad=True)\n",
      "/tmp/ipykernel_3524219/2315991000.py:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, requires_grad=True)\n",
      "/home/yw/anaconda3/envs/newnewcomer/lib/python3.12/site-packages/torch/_tensor.py:1089: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  return self.numpy().astype(dtype, copy=False)\n"
     ]
    }
   ],
   "source": [
    "wlRes = 10\n",
    "\n",
    "wl = np.linspace(500,550, wlRes)#500.0  # wavelength in nm\n",
    "\n",
    "wl = np.reshape(wl,newshape=(wlRes,1))\n",
    "\n",
    "\n",
    "r_core = 80.0\n",
    "r_shell = r_core + 100.0\n",
    "\n",
    "n_env = 1\n",
    "n_core = 4\n",
    "n_shell = 0.1   + .7j\n",
    "\n",
    "mu_env = 1\n",
    "mu_core = 1\n",
    "mu_shell = 1\n",
    "\n",
    "dtype = torch.complex64\n",
    "dtype2 = torch.float64\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "n_max = 5\n",
    "k = 2 * np.pi / (wl / n_env)\n",
    "\n",
    "#print(k)\n",
    "\n",
    "m1 = n_core / n_env\n",
    "m2 = n_shell / n_env\n",
    "\n",
    "K = torch.tensor(k, requires_grad=False, dtype=dtype2)\n",
    "\n",
    "#a = torch.tensor(r_core, requires_grad=True)\n",
    "#b = torch.tensor(r_shell, requires_grad=True)\n",
    "#a = torch.full(K.shape, r_core, requires_grad=True, dtype=dtype)\n",
    "#b = torch.full(K.shape, r_shell, requires_grad=True, dtype=dtype)\n",
    "#m1 = torch.tensor(m1, requires_grad=True, dtype=dtype)\n",
    "#m2 = torch.tensor(m2, requires_grad=True, dtype=dtype)\n",
    "m1 = torch.full(K.shape, m1, requires_grad=True, dtype=dtype)\n",
    "m2 = torch.full(K.shape, m2, requires_grad=True, dtype=dtype)\n",
    "\n",
    "x = K * r_core\n",
    "y = K * r_shell\n",
    "\n",
    "x = torch.tensor(x, requires_grad=True)\n",
    "y = torch.tensor(y, requires_grad=True)\n",
    "\n",
    "n = torch.tensor([1,2,3,4,5], dtype=dtype)\n",
    "\n",
    "#print(n, n.shape)\n",
    "\n",
    "#x_ = x(K, a)\n",
    "#y_ = y(K, b)\n",
    "\n",
    "A = an(x, y, n, m1, m2)\n",
    "B = bn(x, y, n, m1, m2)\n",
    "\n",
    "print(K.shape)\n",
    "\n",
    "print(\"here\",torch.sum((2 * n + 1) * (A.real + B.real),dim=1).shape)\n",
    "\n",
    "qext = torch.mul(K, torch.unsqueeze(torch.sum((2 * n + 1) * (A.real + B.real), dim=1),1))\n",
    "qsca = K * torch.unsqueeze(torch.sum((2 * n + 1) * (A.real**2 + A.imag**2 + B.real**2 + B.imag**2), dim=1),1)\n",
    "qabs = qext - qsca\n",
    "\n",
    "print(qext, qext.shape)\n",
    "print(qsca, qsca.shape)\n",
    "print(qabs, qabs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ddX tensor([[nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj]], grad_fn=<CopyBackwards>)\n",
      "ddY tensor([[nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj]], grad_fn=<CopyBackwards>)\n",
      "ddm1 tensor([[12.8661-8.4589e-04j],\n",
      "        [12.7248-4.2461e-04j],\n",
      "        [12.5865-7.3176e-05j],\n",
      "        [12.4511+2.3557e-04j],\n",
      "        [12.3185+5.2130e-04j],\n",
      "        [12.1888+7.9940e-04j],\n",
      "        [12.0616+1.0820e-03j],\n",
      "        [11.9371+1.3816e-03j],\n",
      "        [11.8150+1.7101e-03j],\n",
      "        [11.6953+2.0816e-03j]], grad_fn=<CopyBackwards>)\n",
      "ddm2 tensor([[nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj],\n",
      "        [nan+nanj]], grad_fn=<CopyBackwards>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yw/anaconda3/envs/newnewcomer/lib/python3.12/site-packages/scipy/special/_spherical_bessel.py:88: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  n = np.asarray(n, dtype=np.dtype(\"long\"))\n",
      "/home/yw/anaconda3/envs/newnewcomer/lib/python3.12/site-packages/scipy/special/_spherical_bessel.py:176: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  n = np.asarray(n, dtype=np.dtype(\"long\"))\n",
      "/tmp/ipykernel_3673673/1728540047.py:83: RuntimeWarning: overflow encountered in multiply\n",
      "  grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n.detach().numpy()**2 - ctx.n.detach().numpy() - grad_output**2)*spherical_yn(ctx.n.detach().numpy(), grad_output) + 2*grad_output*spherical_yn(ctx.n.detach().numpy() + 1, grad_output)) )\n",
      "/tmp/ipykernel_3673673/1728540047.py:83: RuntimeWarning: invalid value encountered in multiply\n",
      "  grad_input = torch.from_numpy( (1/grad_output**2)*((ctx.n.detach().numpy()**2 - ctx.n.detach().numpy() - grad_output**2)*spherical_yn(ctx.n.detach().numpy(), grad_output) + 2*grad_output*spherical_yn(ctx.n.detach().numpy() + 1, grad_output)) )\n"
     ]
    }
   ],
   "source": [
    "qext.backward([x, y, m1, m2], retain_graph = True, create_graph = True)\n",
    "\n",
    "print(\"ddX\", x.grad)\n",
    "print(\"ddY\", y.grad)\n",
    "print(\"ddm1\", m1.grad)\n",
    "print(\"ddm2\", m2.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newnewcomer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
